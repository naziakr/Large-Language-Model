from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-large", cache_dir="new_cache_dir/")
model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-large", cache_dir="new_cache_dir/")

my_text = "explain AI"
inputs = tokenizer(my_text, return_tensors = "pt")
outputs = model.generate(**inputs, \
                         min_length = 256, \
                         max_new_tokens = 512, \
                         length_penalty = 2, \
                         num_beams = 16, \
                         no_repeat_ngram_size = 2, \
                         # num_return_sequence = 2, \
                         early_stopping = True)

output_text_Flan_t5 = tokenizer.batch_decode(outputs, \
                                             skip_special_tokens = True)

print(output_text_Flan_t5 )   
